"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[887],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var o=i(6540);const t={},r=o.createContext(t);function a(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(r.Provider,{value:n},e.children)}},8773:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-4-vla/voice-to-action","title":"Lesson 4.1: Voice-to-Action","description":"Introduction: Bridging Voice and Robotics","source":"@site/docs/04-module-4-vla/01-voice-to-action.md","sourceDirName":"04-module-4-vla","slug":"/module-4-vla/voice-to-action","permalink":"/HACKATHON-I-PHYSICAL-AI-HUMANOID/docs/module-4-vla/voice-to-action","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3.3: Nav2 for Humanoids","permalink":"/HACKATHON-I-PHYSICAL-AI-HUMANOID/docs/module-3-isaac/nav2-for-humanoids"},"next":{"title":"Lesson 4.2: Cognitive Planning","permalink":"/HACKATHON-I-PHYSICAL-AI-HUMANOID/docs/module-4-vla/cognitive-planning"}}');var t=i(4848),r=i(8453);const a={},s="Lesson 4.1: Voice-to-Action",c={},d=[{value:"Introduction: Bridging Voice and Robotics",id:"introduction-bridging-voice-and-robotics",level:2},{value:"OpenAI Whisper for Speech-to-Text",id:"openai-whisper-for-speech-to-text",level:2},{value:"Key Features of Whisper:",id:"key-features-of-whisper",level:3},{value:"Conceptual Workflow:",id:"conceptual-workflow",level:3},{value:"Simple Example (Python with <code>openai-whisper</code> library)",id:"simple-example-python-with-openai-whisper-library",level:2},{value:"How to Use (Requires <code>openai-whisper</code> installation)",id:"how-to-use-requires-openai-whisper-installation",level:3}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-41-voice-to-action",children:"Lesson 4.1: Voice-to-Action"})}),"\n",(0,t.jsx)(n.h2,{id:"introduction-bridging-voice-and-robotics",children:"Introduction: Bridging Voice and Robotics"}),"\n",(0,t.jsx)(n.p,{children:"The ability for robots to understand and act upon spoken commands is a significant step towards more intuitive human-robot interaction (HRI). This involves converting human speech into text (Speech-to-Text), understanding the intent of that text (Natural Language Understanding), and then translating that intent into actionable robot commands."}),"\n",(0,t.jsx)(n.p,{children:"OpenAI Whisper is a general-purpose speech recognition model that can transcribe audio into text. It is trained on a large dataset of diverse audio and is capable of robust speech recognition across various languages and domains."}),"\n",(0,t.jsx)(n.h2,{id:"openai-whisper-for-speech-to-text",children:"OpenAI Whisper for Speech-to-Text"}),"\n",(0,t.jsx)(n.p,{children:"Whisper can be used to accurately transcribe spoken commands, which can then be processed by other systems to generate robot actions."}),"\n",(0,t.jsx)(n.h3,{id:"key-features-of-whisper",children:"Key Features of Whisper:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multilingual"}),": Transcribes in multiple languages and translates into English."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Handles various audio conditions (noise, accents, technical jargon)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Accuracy"}),": Achieves high accuracy on a wide range of speech inputs."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"conceptual-workflow",children:"Conceptual Workflow:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Input"}),": A microphone captures spoken commands."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Whisper Transcription"}),": The audio is fed to the Whisper model, which outputs a text transcription."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Intent Recognition"}),': A Natural Language Understanding (NLU) component analyzes the text to extract the user\'s intent and any relevant parameters (e.g., "go to the kitchen" -> intent: ',(0,t.jsx)(n.code,{children:"navigate"}),", parameter: ",(0,t.jsx)(n.code,{children:"location=kitchen"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action Generation"}),": The recognized intent and parameters are translated into a sequence of robot actions or a ROS 2 command."]}),"\n"]}),"\n",(0,t.jsxs)(n.h2,{id:"simple-example-python-with-openai-whisper-library",children:["Simple Example (Python with ",(0,t.jsx)(n.code,{children:"openai-whisper"})," library)"]}),"\n",(0,t.jsxs)(n.p,{children:["This example demonstrates how to use the ",(0,t.jsx)(n.code,{children:"openai-whisper"})," Python library to transcribe an audio file."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import whisper\nimport os\n\ndef transcribe_audio(audio_path):\n    """\n    Transcribes an audio file using the OpenAI Whisper model.\n    """\n    if not os.path.exists(audio_path):\n        print(f"Error: Audio file not found at {audio_path}")\n        return None\n\n    try:\n        # Load the Whisper model (e.g., \'base\', \'small\', \'medium\', \'large\')\n        # \'base\' is a good starting point for quick transcription\n        model = whisper.load_model("base")\n        print(f"Transcribing audio from: {audio_path}")\n        result = model.transcribe(audio_path)\n        return result["text"]\n    except Exception as e:\n        print(f"An error occurred during transcription: {e}")\n        return None\n\nif __name__ == "__main__":\n    # Create a dummy audio file for demonstration (requires pydub and soundfile)\n    # In a real scenario, you\'d have an actual .wav or .mp3 file\n    try:\n        from pydub import AudioSegment\n        from pydub.generators import WhiteNoise\n        \n        # Generate 3 seconds of white noise\n        noise = WhiteNoise().to_audio_segment(duration=3000)\n        # Add a simple voice saying "Hello robot, go forward" (this is a placeholder and won\'t actually be spoken)\n        # For a real test, you\'d record your voice.\n        \n        # Create a dummy audio file named "command.wav"\n        dummy_audio_path = "command.wav"\n        noise.export(dummy_audio_path, format="wav")\n        print(f"Created a dummy audio file: {dummy_audio_path}")\n\n        # You would replace "command.wav" with your actual audio file\n        transcription = transcribe_audio(dummy_audio_path)\n\n        if transcription:\n            print("\\nTranscription:")\n            print(transcription)\n            # Example of simple intent recognition (very basic)\n            if "go forward" in transcription.lower():\n                print("Intent recognized: Move robot forward")\n            elif "stop" in transcription.lower():\n                print("Intent recognized: Stop robot")\n            else:\n                print("Intent: Unclear command")\n\n        # Clean up the dummy audio file\n        os.remove(dummy_audio_path)\n        print(f"Cleaned up dummy audio file: {dummy_audio_path}")\n\n    except ImportError:\n        print("Please install pydub and soundfile to run the dummy audio generation:")\n        print("pip install pydub soundfile")\n        print("\\nTo transcribe an existing audio file, run this script with your file path:")\n        print("python your_script_name.py /path/to/your/audio.wav")\n    except Exception as e:\n        print(f"An unexpected error occurred: {e}")\n\n'})}),"\n",(0,t.jsxs)(n.h3,{id:"how-to-use-requires-openai-whisper-installation",children:["How to Use (Requires ",(0,t.jsx)(n.code,{children:"openai-whisper"})," installation)"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Install Whisper"}),":","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install -U openai-whisper\n"})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Prepare an audio file"}),": You'll need an audio file (e.g., ",(0,t.jsx)(n.code,{children:".wav"}),", ",(0,t.jsx)(n.code,{children:".mp3"}),") containing spoken commands. For testing, you can record your voice."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Run the script"}),":","\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python your_script_name.py # If using the dummy audio generation\n# OR\npython your_script_name.py /path/to/your/actual_command.wav\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This will output the transcribed text, which can then be used by subsequent robotics modules for action."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}}}]);